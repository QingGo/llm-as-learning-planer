# Week 1-2 每日学习计划

**总学习时长**: 100 小时

## Day 1 Week 1, Day 1
**当日总时长**: 7 小时

**休息时间安排**: 上午、下午各安排15分钟休息；午餐休息1小时。

**学习建议**: 第一天以建立整体认知和回顾基础为主，推导过程遇到卡壳可查阅资料，但务必自己推演一遍。

### 当日任务
#### Transformer理论基础精读 (2小时)
**任务描述**: 精读《Attention Is All You Need》论文原文1-3节，结合The Illustrated Transformer等图解博客，使用笔记软件或纸笔，绘制并理解Transformer的整体架构图，重点厘清Encoder和Decoder在原始论文中的结构与数据流。
**涉及技能**: 文献阅读与理解, Transformer架构总览
**预期成果**: 一份清晰的架构图与对论文1-3节的摘要笔记，能够口头复述Transformer的三大核心创新点。

#### 注意力机制深度推导 (2.5小时)
**任务描述**: 深入理解Scaled Dot-Product Attention公式。在纸上或使用Jupyter Notebook，手动推导Q、K、V矩阵的维度变化、Attention Score的计算以及softmax的作用。理解为何要除以sqrt(d_k)。
**涉及技能**: 数学推导能力, 注意力机制核心
**预期成果**: 能独立写出注意力公式的计算步骤，并清晰解释每个变量的含义及维度变化。

#### PyTorch核心API回顾与实验 (2.5小时)
**任务描述**: 在PyTorch环境中，复习并练习Tensor创建、基本操作、自动求导（autograd）及nn.Module基类的使用。完成一个简单的自定义线性层，并验证其前向传播和反向传播。
**涉及技能**: PyTorch编程, 模型模块化
**预期成果**: 一个可运行的Jupyter Notebook，包含自定义线性层的实现及测试代码。

## Day 2 Week 1, Day 2
**当日总时长**: 7 小时

**休息时间安排**: 每完成一个大任务后休息10-15分钟。

**学习建议**: 实现时优先保证逻辑正确和清晰，性能优化可后续进行。强烈建议为每个函数和类编写简单的测试用例。

### 当日任务
#### Multi-Head Attention原理与实现拆解 (2小时)
**任务描述**: 深入理解Multi-Head Attention的动机与实现方式。精读论文对应章节，并分析Hugging Face源码中`BertSelfAttention`类的实现逻辑。在笔记中画出多头注意力中线性投影、分头、计算、拼接、输出的完整数据流图。
**涉及技能**: 多头注意力机制, 源码阅读
**预期成果**: 一份详细的数据流图和多头注意力分步计算说明笔记。

#### 动手实现Self-Attention模块（上） (3小时)
**任务描述**: 使用纯PyTorch（不使用nn.MultiheadAttention），从零开始实现Scaled Dot-Product Attention函数。要求函数能正确处理掩码（mask）。编写单元测试，使用随机数据验证输出形状的正确性。
**涉及技能**: PyTorch编程, 模块化实现
**预期成果**: 一个通过单元测试的、可复用的SelfAttention函数模块（py文件）。

#### 动手实现Self-Attention模块（下） (2小时)
**任务描述**: 基于上一步的SelfAttention函数，实现完整的MultiHeadAttention类。该类应包含投影层、分头/合并操作，并调用你的SelfAttention函数。编写单元测试验证其功能。
**涉及技能**: PyTorch编程, 模块化实现, 面向对象设计
**预期成果**: 一个通过单元测试的、可复用的MultiHeadAttention类（py文件）。

## Day 3 Week 1, Day 3
**当日总时长**: 7 小时

**休息时间安排**: 下午开始前可安排稍长休息（20分钟）以切换思维。

**学习建议**: 今天是集成日，确保每个子模块都正确后再进行组装。画出编码器层的结构图有助于编码。

### 当日任务
#### 位置编码（Positional Encoding）研究 (2小时)
**任务描述**: 理解绝对位置编码（正弦/余弦）的公式及其设计思想。阅读关于相对位置编码的简介（如Transformer-XL、RoPE）。动手实现正弦位置编码函数，并可视化前50个位置、不同维度的编码值。
**涉及技能**: 位置编码理论, PyTorch编程, 数据可视化
**预期成果**: 实现的位置编码函数及可视化结果图，理解其如何携带顺序信息。

#### 前馈网络与残差连接 (2.5小时)
**任务描述**: 实现Transformer中的Position-wise Feed-Forward Network（FFN）。理解并实现残差连接（Residual Connection）和层归一化（Layer Normalization）。思考并记录它们对深层网络训练稳定性的作用。
**涉及技能**: 网络模块实现, 深度学习基础
**预期成果**: 实现FFN、残差连接与LayerNorm的模块，并整合成一个`TransformerEncoderLayer`的草稿。

#### 构建Transformer编码器层 (2.5小时)
**任务描述**: 整合前两天的MultiHeadAttention模块和今天的FFN、Add & Norm模块，构建一个完整的`TransformerEncoderLayer`类。编写测试，用随机输入验证该层的正向传播。
**涉及技能**: 模块集成, PyTorch编程
**预期成果**: 一个功能完整、可通过正向传播测试的`TransformerEncoderLayer`类。

## Day 4 Week 1, Day 4
**当日总时长**: 7.5 小时

**休息时间安排**: 任务间穿插短休息，保持专注力。

**学习建议**: 模型整合后，用一个小batch的数据跑通前向传播，这是重要的冒烟测试。数据加载是工程关键，确保输出格式与模型输入匹配。

### 当日任务
#### 完成Transformer编码器堆叠 (2小时)
**任务描述**: 实现`TransformerEncoder`类，它由N个`TransformerEncoderLayer`堆叠而成，并包含输入嵌入层和位置编码的添加。思考并实现可选的padding mask生成与传递逻辑。
**涉及技能**: 架构集成, 掩码处理
**预期成果**: 完整的`TransformerEncoder`类，能处理变长序列输入（通过mask）。

#### 数据集准备与探索（SST-2） (2小时)
**任务描述**: 从GLUE官网或Hugging Face Datasets库下载SST-2数据集。使用Pandas或datasets库进行数据探索：查看样本格式、统计序列长度分布、检查标签分布。
**涉及技能**: 数据处理, 数据集理解
**预期成果**: 一个Jupyter Notebook，包含数据的基本统计信息和几个样例展示。

#### 构建文本分类头与模型整合 (2小时)
**任务描述**: 设计并实现用于SST-2二分类任务的模型头部（例如，使用[CLS]位置的输出接一个线性层）。将`TransformerEncoder`与分类头整合成完整的`TransformerForSequenceClassification`模型。
**涉及技能**: 任务头设计, 模型整合
**预期成果**: 完整的分类模型类定义，结构清晰，职责明确。

#### 实现基础数据加载器 (1.5小时)
**任务描述**: 编写文本分词函数（可使用简单空格分词或`torchtext`的basic_english）。实现一个简单的`Dataset`和`DataLoader`，能够将文本转换为词索引序列，并生成对应的batch（包含padding和mask）。
**涉及技能**: PyTorch DataLoader, 数据预处理
**预期成果**: 可以成功迭代出batch数据的数据加载管道。

## Day 5 Week 1, Day 5
**当日总时长**: 7.5 小时

**休息时间安排**: 下午可安排较长的总结休息时间。

**学习建议**: 第一个能跑的训练循环是重要里程碑，不必追求性能，关键是流程正确。遇到问题善用print和调试器。

### 当日任务
#### 编写基础训练循环 (3小时)
**任务描述**: 实现一个最简单的训练循环：包括损失函数（CrossEntropyLoss）、优化器（Adam）的选择，以及一个epoch的训练步骤（前向、损失计算、反向传播、参数更新）。在极小数据集（如100条）上运行，确保损失下降。
**涉及技能**: 训练流程实现, PyTorch优化
**预期成果**: 一个能运行、损失函数值在下降的基础训练脚本。

#### 集成评估函数 (2小时)
**任务描述**: 实现模型评估函数，在验证集上计算准确率（Accuracy）。修改训练循环，在每个epoch后打印训练损失和验证准确率。
**涉及技能**: 模型评估, 代码组织
**预期成果**: 训练脚本具备简单的训练-验证评估流程，并能输出准确率。

#### Week 1 总结与问题修复 (2.5小时)
**任务描述**: 回顾本周所有代码，检查模块接口是否清晰、有无bug。运行完整的1个epoch训练，观察是否存在梯度爆炸/消失（检查loss是否NaN）。整理本周遇到的主要问题和解决方案。
**涉及技能**: 调试, 知识复盘
**预期成果**: 一份问题记录文档，以及一个可稳定运行1个epoch的训练代码版本。

## Day 6 Week 1, Day 6
**当日总时长**: 7.5 小时

**休息时间安排**: 周末学习可适当宽松，保证充足休息。

**学习建议**: 工程化是体现专业性的关键一步，良好的结构让后续实验和调试更轻松。

### 当日任务
#### 学习优化技术：梯度累积 (2小时)
**任务描述**: 研究梯度累积的原理（模拟大batch size）。修改训练循环，实现梯度累积逻辑。思考并记录其在显存受限时的应用价值。
**涉及技能**: 训练优化技巧
**预期成果**: 理解梯度累积的代码实现方式，并能在自己的训练脚本中实现。

#### 学习优化技术：混合精度训练（AMP） (2.5小时)
**任务描述**: 学习PyTorch的自动混合精度（AMP）工具（`torch.cuda.amp`）。理解FP16训练的优势与风险（梯度下溢）。修改训练循环，集成AMP的autocast和梯度缩放。
**涉及技能**: 混合精度训练, 性能优化
**预期成果**: 训练脚本支持混合精度训练，并了解其基本使用方法。

#### 代码重构与工程化 (3小时)
**任务描述**: 将模型定义、数据处理、训练循环等代码重构到不同的模块文件中（如`model.py`, `data.py`, `train.py`）。使用配置文件（如YAML）或参数解析器（argparse）管理超参数。
**涉及技能**: 代码工程化, 项目管理
**预期成果**: 项目结构清晰，模块分离，超参数可配置。

## Day 7 Week 1, Day 7
**当日总时长**: 7 小时

**休息时间安排**: 本周日为调整和缓冲日，任务量较少，以总结和规划为主。

**学习建议**: 通过实验验证模型实现的有效性。如果准确率远低于预期（如<80%），需要回头检查模型或数据处理是否存在根本性错误。

### 当日任务
#### 深度调参与实验分析（目标：验证实现） (4小时)
**任务描述**: 设计一个小的超参数搜索实验（如调整学习率、隐藏层维度、层数）。使用你的工程化代码，运行3-4组不同配置的实验，记录每个实验的最终验证集准确率。
**涉及技能**: 实验设计, 超参数调优
**预期成果**: 实验记录表格，包含不同配置的性能对比。

#### 项目文档与Git提交 (3小时)
**任务描述**: 为你的手撕Transformer项目撰写详细的README.md，包括项目介绍、环境依赖、代码结构、如何运行以及初步实验结果。初始化Git仓库，进行第一次正式提交。
**涉及技能**: 文档撰写, 版本控制
**预期成果**: 专业的README文档和一个干净的Git初始提交。

## Day 8 Week 2, Day 1
**当日总时长**: 7.5 小时

**休息时间安排**: 进入第二周，继续保持节奏，上午开始前做好计划。

**学习建议**: 本周聚焦工程优化和深度训练，目标是让模型性能达到90%+。可视化是分析和展示的关键。

### 当日任务
#### 学习PyTorch自定义Dataset高级特性 (2.5小时)
**任务描述**: 深入研究PyTorch的`Dataset`和`DataLoader`，实现更高效的数据处理，如动态padding（使用`collate_fn`）、预加载到内存等。优化SST-2的数据加载流程。
**涉及技能**: 高效数据加载
**预期成果**: 一个高效、支持动态批次处理的`DataLoader`。

#### 实现学习率调度器 (2小时)
**任务描述**: 学习并实现常见的学习率调度策略，如带有热身的线性衰减（Linear Decay with Warmup）。将其集成到训练循环中。
**涉及技能**: 学习率调度
**预期成果**: 训练脚本支持可配置的学习率热身与衰减。

#### 模型训练稳定性与监控 (3小时)
**任务描述**: 在训练循环中添加更多监控指标：梯度范数、权重范数、学习率变化曲线。确保训练过程稳定（无NaN，损失平滑下降）。尝试使用TensorBoard或WandB记录损失和准确率曲线。
**涉及技能**: 训练监控, 实验跟踪
**预期成果**: 训练脚本集成了可视化工具，能够监控训练动态。

## Day 9 Week 2, Day 2
**当日总时长**: 7.5 小时

**休息时间安排**: 长时间训练时，可以趁GPU运行期间休息或处理其他学习任务。

**学习建议**: 这是检验前8天成果的关键时刻。耐心观察训练过程，准确率是核心KPI。

### 当日任务
#### 目标冲刺：完整训练与性能优化 (6小时)
**任务描述**: 使用所有已实现的优化（AMP、梯度累积、学习率调度、高效DataLoader），在完整的SST-2训练集上启动一次长时间训练（例如10-20个epoch）。目标是验证集准确率稳定达到90%以上。
**涉及技能**: 端到端训练, 性能调优
**预期成果**: 一份训练日志和验证准确率达到或接近90%的模型检查点。

#### 错误分析与模型调试 (1.5小时)
**任务描述**: 分析模型在验证集上分错的样本。是模型容量不足？过拟合？还是数据噪音？根据分析思考可能的改进方向（如增加dropout，调整模型大小）。
**涉及技能**: 模型评估与调试
**预期成果**: 一份简短的错误分析报告，列出几个典型错误案例及分析。

## Day 10 Week 2, Day 3
**当日总时长**: 7.5 小时

**休息时间安排**: 收尾工作细致，保持耐心。

**学习建议**: 好的文档和报告是项目价值的倍增器，也是未来求职时的重要展示材料。

### 当日任务
#### 模型测试与指标计算 (2小时)
**任务描述**: 在预留的SST-2测试集上评估最佳模型。计算最终的测试准确率，并确保与验证集结果接近，以检查过拟合。编写生成测试报告的脚本。
**涉及技能**: 模型测试, 性能报告
**预期成果**: 在测试集上的准确率报告（目标：≥90%）。

#### 项目收尾：代码整理与注释 (2.5小时)
**任务描述**: 为所有关键函数和类添加清晰的文档字符串（docstring）和行内注释。确保代码符合PEP8等基本规范（可借助Black、isort等工具）。
**涉及技能**: 代码规范, 文档化
**预期成果**: 一份注释清晰、格式规范的完整项目代码。

#### 项目收尾：撰写最终实验报告 (3小时)
**任务描述**: 撰写一份简明的最终报告，包括：项目目标、模型架构图、关键实现细节、超参数设置、训练曲线图（损失/准确率）、最终测试结果以及遇到的挑战与解决方案。
**涉及技能**: 技术写作, 项目总结
**预期成果**: 一份结构完整、图文并茂的项目总结报告（Markdown或PDF格式）。

## Day 11 Week 2, Day 4
**当日总时长**: 7.5 小时

**休息时间安排**: 可选任务可根据自身掌握情况调整时间分配。

**学习建议**: 今天以深化和拓展为主，为从“手写实现”过渡到“工业级框架应用”打下更扎实的基础。

### 当日任务
#### 知识深化：反向传播推导（选做/挑战） (3小时)
**任务描述**: 针对你实现的Transformer编码器层，尝试在纸上推导其关键部分（如Self-Attention）的反向传播公式。理解梯度是如何通过注意力权重传播回Q、K、V的。
**涉及技能**: 深度学习理论, 数学能力
**预期成果**: 对Transformer反向传播有更深刻的理解，能大致描述梯度流。

#### 拓展学习：Hugging Face源码对照 (3小时)
**任务描述**: 将你手写的Transformer编码器与Hugging Face Transformers库中`BertEncoder`的实现进行逐行对照。学习其工业级的代码风格、配置管理和性能优化技巧。
**涉及技能**: 源码学习, 工程实践参考
**预期成果**: 一份对比笔记，记录HF实现中的优秀设计和可借鉴之处。

#### 准备Week 3-4的预学习 (1.5小时)
**任务描述**: 预先浏览双周计划中Week 3-4的内容（Hugging Face生态）。安装`transformers`, `datasets`, `tokenizers`库，并快速运行官方Quicktour，了解基本用法。
**涉及技能**: 前瞻性学习
**预期成果**: 成功安装HF相关库，并对下一阶段内容有初步印象。

## Day 12 Week 2, Day 5
**当日总时长**: 8 小时

**休息时间安排**: 本周最后一天，以总结和规划为主，放松心态。

**学习建议**: 复盘和规划同样重要。庆祝完成第一个里程碑，清晰地看到自己从理论到实现的完整闭环。

### 当日任务
#### Week 1-2 全面复盘 (2小时)
**任务描述**: 对照第一周开始时绘制的架构图，回顾整个实现过程。回答核心问题：Self-Attention如何工作？多头注意力的意义？位置编码的作用？残差和LayerNorm的作用？
**涉及技能**: 知识体系构建, 复盘
**预期成果**: 能够流畅、自信地回答上述所有核心问题。

#### GitHub仓库最终优化 (2小时)
**任务描述**: 确保GitHub仓库包含所有必要文件：代码、README、实验报告、训练曲线图、结果截图。检查README的引导是否清晰，确保他人可以复现你的结果。
**涉及技能**: 项目展示
**预期成果**: 一个整洁、专业、可复现的GitHub项目仓库。

#### 制定Week 3-4详细学习计划 (2小时)
**任务描述**: 基于双周计划概要和今天的预学习，为自己制定接下来两周（Week 3-4）的每日学习计划草案。明确学习重点和预期项目产出。
**涉及技能**: 学习规划
**预期成果**: 一份初步的Week 3-4每日学习计划草案。

#### 缓冲与休息 (2小时)
**任务描述**: 处理前11天遗留的小问题，或进行放松休息，为下一个双周任务储备精力。
**涉及技能**: 时间管理, 精力恢复
**预期成果**: 身心得到放松，准备好进入下一阶段。

## 双周学习总结和回顾建议
**双周回顾建议：**
1.  **成果检查**：是否拥有一个可运行、注释清晰、测试准确率≥90%的“手撕Transformer”代码仓库？是否撰写了项目总结报告？
2.  **知识自测**：能否在不看代码的情况下，在白板上画出Transformer编码器架构，并解释Self-Attention、Multi-Head、FFN、Add & Norm的作用？能否说明训练中使用的优化技巧（AMP、梯度累积）的目的？
3.  **技能评估**：对PyTorch的Module、Dataset、DataLoader、Optimizer的使用是否熟练？是否体验了完整的模型训练、验证、测试流程？
4.  **问题归档**：将这两周遇到并解决的所有关键技术问题（如维度错误、梯度异常、性能瓶颈）及解决方案记录下来，形成你的个人“错题本”，这在面试准备中是宝贵财富。
5.  **调整方向**：如果最终准确率未达预期，分析是模型实现错误、数据处理问题还是训练策略不当？计划一天时间进行针对性调试。如果提前高质量完成所有目标，可以利用富余时间提前深入Week 3的内容或进一步优化本项目性能。