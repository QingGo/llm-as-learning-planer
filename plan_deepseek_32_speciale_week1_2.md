# Week 1-2 每日学习计划

**总学习时长**: 100 小时

## Day 1 Week 1, Day 1
**当日总时长**: 8 小时

**休息时间安排**: 建议每工作1-1.5小时休息10分钟，中午安排1小时午餐休息。

**学习建议**: 动手实践为主，阅读文档时记笔记；遇到问题及时查阅官方文档或Stack Overflow。

### 当日任务
#### 复习PyTorch基础 (1小时)
**任务描述**: 回顾Tensor操作、自动微分、简单模型构建。通过编写一个简单的全连接网络分类MNIST数据集来巩固基础。
**涉及技能**: PyTorch基础
**预期成果**: 能够独立编写一个简单的神经网络分类模型，并理解前向和反向传播过程。

#### 深入学习DataLoader和Dataset (2小时)
**任务描述**: 阅读PyTorch官方教程中Data Loading部分，理解Dataset抽象、DataLoader参数、Sampler、collate_fn。编写一个自定义Dataset类加载文本文件（例如WikiText-2），并利用DataLoader生成批次。
**涉及技能**: DataLoader, Dataset
**预期成果**: 实现一个自定义Dataset，能够从文本文件读取数据，并可通过DataLoader迭代返回批次数据。

#### 自定义nn.Module和参数初始化 (1.5小时)
**任务描述**: 学习如何定义复杂的nn.Module子类，使用ModuleList等容器。掌握常用参数初始化方法（如xavier_uniform_、kaiming_normal_）。实现一个多层感知机（MLP）并正确初始化参数。
**涉及技能**: PyTorch模型构建, 参数初始化
**预期成果**: 完成MLP模型类，包含自定义初始化，并可通过前向传播测试。

#### 混合精度训练原理与实践 (1.5小时)
**任务描述**: 阅读PyTorch AMP文档，理解自动混合精度训练的原理，掌握torch.cuda.amp.autocast和GradScaler的使用。
**涉及技能**: 混合精度训练
**预期成果**: 能够解释混合精度如何节省显存和加速训练，并能在训练循环中添加几行代码启用混合精度。

#### 综合练习：用混合精度训练MLP (2小时)
**任务描述**: 使用CIFAR-10数据集，构建DataLoader，定义MLP模型，使用混合精度训练几个epoch，记录训练损失，验证准确率。
**涉及技能**: 综合应用, 混合精度训练, DataLoader
**预期成果**: 成功训练MLP，验证准确率约50%，并保存代码；理解混合精度对训练的影响。

## Day 2 Week 1, Day 2
**当日总时长**: 8 小时

**休息时间安排**: 每工作1小时休息5-10分钟，中午休息1小时。

**学习建议**: 结合论文和代码理解，多动手实现；对于数学推导，可以写在纸上加深记忆。

### 当日任务
#### 阅读Transformer论文 (1.5小时)
**任务描述**: 精读'Attention Is All You Need'论文的3.1-3.5节，理解Transformer架构的编码器、解码器、自注意力、多头注意力、位置编码、前馈网络等组件。
**涉及技能**: Transformer理论
**预期成果**: 能够描述Transformer的整体结构，并解释每个组件的功能及数学公式。

#### 学习The Annotated Transformer (2小时)
**任务描述**: 阅读博客'The Annotated Transformer'，理解PyTorch实现细节，重点关注自注意力、多头注意力和位置编码的代码实现。
**涉及技能**: Transformer实现
**预期成果**: 理解Transformer各模块的代码实现，并能与论文对应。

#### 深入理解自注意力机制 (1.5小时)
**任务描述**: 推导自注意力计算过程，包括QKV投影、缩放点积、softmax、加权求和。手动计算一个简单例子（如序列长度2，嵌入维度4）。
**涉及技能**: 注意力机制
**预期成果**: 能够清晰地解释自注意力机制，并手动完成计算，验证理解。

#### 位置编码与层归一化 (1小时)
**任务描述**: 学习正弦位置编码的原理和代码实现；理解层归一化的作用和实现。编写一个位置编码函数，并解释为什么Transformer使用层归一化。
**涉及技能**: 位置编码, 层归一化
**预期成果**: 实现正弦位置编码函数，能对任意序列长度生成位置编码；理解LayerNorm对训练稳定的作用。

#### 实现单头自注意力模块 (2小时)
**任务描述**: 在PyTorch中实现一个单头自注意力模块（SelfAttention），支持mask（用于decoder），输入输出形状均为(batch, seq_len, d_model)。编写测试验证正确性。
**涉及技能**: PyTorch编码, 自注意力实现
**预期成果**: 完成SelfAttention类，并通过随机输入测试输出形状和数值合理性。

## Day 3 Week 1, Day 3
**当日总时长**: 8 小时

**休息时间安排**: 每完成一个模块休息片刻，中午休息1小时。

**学习建议**: 保持耐心，仔细检查维度匹配；多参考nanoGPT的实现，但尽量自己编写。

### 当日任务
#### 研究nanoGPT代码 (1小时)
**任务描述**: 克隆nanoGPT仓库，阅读model.py，了解GPT2模型的结构，注意超参数的含义（如n_layer, n_head, n_embd）。
**涉及技能**: 代码阅读, GPT架构
**预期成果**: 列出GPT模型的组件清单，并理解每个超参数的作用。

#### 搭建GPT模型骨架 (1小时)
**任务描述**: 创建自己的GPT类，定义__init__方法接收配置参数（vocab_size, n_embd, n_head, n_layer等），初始化嵌入层和输出层占位符。
**涉及技能**: PyTorch模型设计
**预期成果**: GPT类的框架代码完成，可以实例化。

#### 实现嵌入层 (1小时)
**任务描述**: 在GPT类中添加token嵌入（nn.Embedding）和位置嵌入（可学习或正弦），实现dropout。
**涉及技能**: 嵌入层
**预期成果**: 嵌入层能根据输入idx返回形状为(batch, seq_len, n_embd)的张量。

#### 实现Transformer Decoder Block (3小时)
**任务描述**: 构建DecoderBlock类，包含层归一化、多头自注意力（带因果mask）、残差连接、前馈网络（两个线性层+GELU）、dropout。参考nanoGPT实现。
**涉及技能**: Transformer block实现
**预期成果**: 完成DecoderBlock，并通过测试。

#### 组合模型前向传播 (1小时)
**任务描述**: 在GPT类中，将多个DecoderBlock串联，添加最终的层归一化和线性输出头，实现前向传播。
**涉及技能**: 模型整合
**预期成果**: GPT模型的前向方法能接收输入ids，返回logits。

#### 测试模型 (1小时)
**任务描述**: 生成随机输入，检查输出形状，计算参数量，确保没有错误。
**涉及技能**: 调试
**预期成果**: 模型运行无报错，输出形状(batch, seq_len, vocab_size)，参数量符合预期。

## Day 4 Week 1, Day 4
**当日总时长**: 8 小时

**休息时间安排**: 每2小时休息10分钟，中午休息1小时。

**学习建议**: 注意数据处理的效率，合理设置num_workers；确保Dataset的__getitem__正确返回。

### 当日任务
#### 获取WikiText-2数据集 (0.5小时)
**任务描述**: 从HuggingFace datasets或官方源下载WikiText-2数据集，了解其目录结构和内容。
**涉及技能**: 数据集获取
**预期成果**: 数据集保存在本地指定目录。

#### 分词器选择与使用 (2小时)
**任务描述**: 学习使用HuggingFace tokenizers库，加载GPT-2分词器（或训练自己的BPE），将文本转换为token ids。
**涉及技能**: 分词器
**预期成果**: 能够对任意文本进行编码和解码，得到token id序列。

#### 构建Dataset类 (2小时)
**任务描述**: 编写一个PyTorch Dataset，读取文本文件，使用分词器编码，并切成固定长度（如block_size）的样本。对于训练数据，生成输入和标签（labels为输入右移一位）。
**涉及技能**: Dataset构建
**预期成果**: Dataset能够返回(input_ids, target_ids)对，长度符合block_size。

#### DataLoader配置 (1.5小时)
**任务描述**: 创建DataLoader，设置batch_size、shuffle、num_workers等，测试迭代速度和数据形状。
**涉及技能**: DataLoader
**预期成果**: DataLoader能够高效生成批次，输出形状(batch, seq_len)。

#### 验证集准备 (1小时)
**任务描述**: 对验证集文本进行相同处理，创建验证集DataLoader。
**涉及技能**: 数据划分
**预期成果**: 验证集DataLoader就绪，用于评估。

#### 检查数据流 (1小时)
**任务描述**: 取一个批次数据传入模型，计算损失，确保无错误。
**涉及技能**: 调试
**预期成果**: 模型能处理真实数据，损失可计算。

## Day 5 Week 1, Day 5
**当日总时长**: 8 小时

**休息时间安排**: 每1小时休息5分钟，中午休息1小时。

**学习建议**: 确保代码无误后再启动长时训练；可利用晚上时间继续训练。

### 当日任务
#### 设计损失函数 (0.5小时)
**任务描述**: 使用nn.CrossEntropyLoss，忽略padding token（如果有）。注意将logits形状调整为(batch*seq_len, vocab_size)，labels为(batch*seq_len)。
**涉及技能**: 损失函数
**预期成果**: 损失函数定义正确。

#### 设置优化器 (0.5小时)
**任务描述**: 使用torch.optim.AdamW，设置学习率（如6e-4）、betas=(0.9,0.999)、权重衰减（0.1）。
**涉及技能**: 优化器
**预期成果**: 优化器实例创建。

#### 学习率调度 (1小时)
**任务描述**: 实现带warmup的余弦退火调度。例如，前10%的step线性增加学习率，之后余弦下降。
**涉及技能**: 学习率调度
**预期成果**: 能够在每个step更新学习率。

#### 构建训练循环 (1小时)
**任务描述**: 编写epoch循环，包括梯度清零、前向传播、损失计算、反向传播、优化器step、学习率调度step。
**涉及技能**: 训练循环
**预期成果**: 训练循环代码完成。

#### 添加日志 (1小时)
**任务描述**: 使用tqdm显示进度条，打印每个batch的loss；可选配置wandb记录。
**涉及技能**: 日志记录
**预期成果**: 训练时能实时看到loss。

#### 编写验证函数 (1小时)
**任务描述**: 在验证集上计算平均损失，并转换为困惑度（exp(loss)）。
**涉及技能**: 评估
**预期成果**: 验证函数返回困惑度。

#### 调试训练 (2小时)
**任务描述**: 在1%的训练数据上运行1个epoch，检查loss是否下降，有无NaN，确保代码无误。
**涉及技能**: 调试
**预期成果**: 小规模训练成功，loss下降。

#### 启动第一次完整训练 (1小时)
**任务描述**: 使用全部训练数据，设置合理epoch数（如10），启动训练，监控初始日志。
**涉及技能**: 训练启动
**预期成果**: 训练正常运行，初步loss趋势正常。

## Day 6 Week 1, Day 6 (Saturday)
**当日总时长**: 8 小时

**休息时间安排**: 每1.5小时休息10分钟，中午休息1小时。

**学习建议**: 多GPU调试可能需要耐心，确保所有进程同步；参考官方示例。

### 当日任务
#### 分析训练结果 (1小时)
**任务描述**: 查看训练和验证损失曲线，计算当前验证困惑度，判断是否过拟合/欠拟合，确定调整方向。
**涉及技能**: 结果分析
**预期成果**: 明确模型表现及改进点。

#### 超参数调整 (1小时)
**任务描述**: 根据分析调整学习率、batch size、模型大小、dropout等，并重新启动训练。
**涉及技能**: 超参数调整
**预期成果**: 新的训练配置就绪。

#### 学习PyTorch DDP (2小时)
**任务描述**: 阅读PyTorch官方DDP教程，理解多进程模型并行原理，掌握DistributedDataParallel的基本用法。
**涉及技能**: 分布式训练
**预期成果**: 能解释DDP的工作机制。

#### 实现DDP支持 (3小时)
**任务描述**: 修改训练脚本以支持多GPU：初始化进程组（使用torch.distributed），包装模型为DDP，使用DistributedSampler，调整梯度累积和保存逻辑。
**涉及技能**: DDP编码
**预期成果**: 脚本可以在多GPU上运行，训练结果与单GPU一致。

#### 集成混合精度 (1小时)
**任务描述**: 将torch.cuda.amp与DDP结合，确保梯度缩放正确。
**涉及技能**: 混合精度与DDP
**预期成果**: AMP在DDP环境下正常工作。

## Day 7 Week 1, Day 7 (Sunday)
**当日总时长**: 2 小时

**休息时间安排**: 自由安排休息。

**学习建议**: 利用周日放松，保持学习节奏。

### 当日任务
#### 观看CS224N讲座 (1.5小时)
**任务描述**: 观看斯坦福CS224N 2019关于Transformer和BERT的讲座（约1.5小时），加深理解。
**涉及技能**: 理论知识
**预期成果**: 笔记记录关键点，理解Transformer在NLP中的角色。

#### 周总结与规划 (0.5小时)
**任务描述**: 回顾本周学习内容，整理笔记，列出已掌握和待加强的部分，规划下周重点。
**涉及技能**: 总结
**预期成果**: 清晰的周总结文档和下周计划。

## Day 8 Week 2, Day 1 (Monday)
**当日总时长**: 8 小时

**休息时间安排**: 每1小时休息5分钟，中午休息1小时。

**学习建议**: 梯度累积时注意loss需要取平均显示；保存最佳模型时注意文件命名。

### 当日任务
#### 检查训练进度 (1小时)
**任务描述**: 评估当前模型在验证集上的困惑度，若达到目标（<55）则准备部署；否则分析原因。
**涉及技能**: 评估
**预期成果**: 了解当前模型状态。

#### 模型调优 (2小时)
**任务描述**: 根据分析调整模型结构或训练参数（如增加dropout、调整学习率、增加模型尺寸），并启动新一轮训练。
**涉及技能**: 模型调优
**预期成果**: 新的训练配置并启动。

#### 实现梯度累积 (1小时)
**任务描述**: 修改训练循环，支持梯度累积（accumulation_steps），模拟更大batch size，正确处理梯度平均和优化器step。
**涉及技能**: 梯度累积
**预期成果**: 脚本支持accumulation_steps，训练结果稳定。

#### 记录与可视化 (1小时)
**任务描述**: 配置TensorBoard或Weights & Biases，记录损失、学习率、困惑度等指标。
**涉及技能**: 可视化
**预期成果**: 能够实时查看训练曲线。

#### 实现模型保存 (1小时)
**任务描述**: 添加检查点保存逻辑，例如每N步保存一次，并保存最佳模型（根据验证困惑度）。
**涉及技能**: 模型持久化
**预期成果**: 训练过程中能自动保存和加载检查点。

#### 训练监控与调试 (2小时)
**任务描述**: 监控训练过程，确保损失正常下降，处理可能出现的NaN或震荡。
**涉及技能**: 监控, 调试
**预期成果**: 训练稳定进行。

## Day 9 Week 2, Day 2 (Tuesday)
**当日总时长**: 8 小时

**休息时间安排**: 每1.5小时休息10分钟，中午休息1小时。

**学习建议**: 多GPU训练时注意batch size的分配；确保DistributedSampler正确shuffle。

### 当日任务
#### 深入理解DDP原理 (1小时)
**任务描述**: 复习DDP文档，了解ring-allreduce算法，模型同步机制，以及如何避免冗余计算。
**涉及技能**: 分布式原理
**预期成果**: 能解释DDP如何同步梯度。

#### 多GPU训练实践 (2小时)
**任务描述**: 如果有至少2块GPU，运行多GPU训练，记录训练吞吐量（tokens per second），并与单卡比较。
**涉及技能**: 多GPU训练
**预期成果**: 获得加速比数据，理解多卡扩展效率。

#### 学习多节点配置 (1小时)
**任务描述**: 了解多节点训练的配置方法，包括环境变量（MASTER_ADDR, MASTER_PORT）和启动命令（torch.distributed.launch）。
**涉及技能**: 多节点配置
**预期成果**: 理解多节点训练的基本设置。

#### 调试DDP常见问题 (2小时)
**任务描述**: 模拟并解决DDP中常见问题，如进程挂起、数据不平衡、sampler设置等。
**涉及技能**: 调试
**预期成果**: 能够诊断和解决典型DDP错误。

#### 集成梯度累积与DDP (1小时)
**任务描述**: 确保梯度累积与DDP兼容，在累积步数内不执行优化器step，只在累积结束后执行并同步梯度。
**涉及技能**: 梯度累积, DDP
**预期成果**: 脚本在多卡下正确进行梯度累积。

#### 分析显存占用 (1小时)
**任务描述**: 使用nvidia-smi或torch.cuda.memory_allocated观察各GPU显存使用，尝试调整模型大小或并行策略以优化显存。
**涉及技能**: 显存分析
**预期成果**: 了解模型各部分的显存占用，能提出优化方案。

## Day 10 Week 2, Day 3 (Wednesday)
**当日总时长**: 8 小时

**休息时间安排**: 每1小时休息5分钟，中午休息1小时。

**学习建议**: 性能优化需逐步进行，每次改动后验证正确性和速度提升；注意不要引入错误。

### 当日任务
#### 复习混合精度原理 (1小时)
**任务描述**: 阅读混合精度最佳实践文档，理解哪些操作需要保持float32，以及如何避免精度问题。
**涉及技能**: 混合精度
**预期成果**: 深入理解混合精度的工作细节。

#### AMP与DDP集成 (1小时)
**任务描述**: 将AMP与DDP无缝结合，检查是否出现精度损失或NaN，确保GradScaler在多进程中正确使用。
**涉及技能**: AMP, DDP
**预期成果**: AMP在DDP环境下运行稳定。

#### 性能剖析 (2小时)
**任务描述**: 使用torch.profiler对训练步骤进行剖析，找出耗时最多的操作（如注意力计算、矩阵乘法、数据加载）。
**涉及技能**: 性能分析
**预期成果**: 生成剖析报告，识别瓶颈。

#### 优化数据加载 (1小时)
**任务描述**: 根据剖析结果，优化数据加载：增加num_workers，启用pin_memory，使用prefetch。
**涉及技能**: 数据加载优化
**预期成果**: 数据加载时间减少，GPU利用率提高。

#### 优化模型计算 (2小时)
**任务描述**: 尝试使用torch.jit编译部分模块，或使用融合算子（如F.scaled_dot_product_attention），优化矩阵乘法等。
**涉及技能**: 计算优化
**预期成果**: 训练迭代时间缩短。

#### 启动优化后的训练 (1小时)
**任务描述**: 应用所有优化，启动最终训练，记录优化前后的吞吐量对比。
**涉及技能**: 综合应用
**预期成果**: 训练速度提升，且结果不变。

## Day 11 Week 2, Day 4 (Thursday)
**当日总时长**: 8 小时

**休息时间安排**: 每2小时休息15分钟，中午休息1小时。

**学习建议**: 生成时注意设置合适的温度（temperature）和采样参数，避免重复或无意义输出。

### 当日任务
#### 监控训练进度 (1小时)
**任务描述**: 查看训练损失和验证困惑度曲线，判断是否收敛。
**涉及技能**: 监控
**预期成果**: 确定模型是否已经训练充分。

#### 验证困惑度评估 (2小时)
**任务描述**: 在验证集上计算最终困惑度，如果低于55则达标；否则分析原因。
**涉及技能**: 评估
**预期成果**: 得到当前困惑度数值，判断是否达标。

#### 模型微调（如未达标） (2小时)
**任务描述**: 若未达标，尝试微调：调整学习率、增加训练epoch、增加模型容量、调整dropout等，并重新训练。
**涉及技能**: 超参数调整
**预期成果**: 改进模型，使困惑度下降。

#### 编写推理代码 (2小时)
**任务描述**: 实现文本生成函数，使用top-k或top-p采样，支持给定prompt生成后续文本。
**涉及技能**: 文本生成
**预期成果**: 能够输入一段文本，模型生成连贯的续写。

#### 生成示例并评估 (1小时)
**任务描述**: 使用训练好的模型生成至少5个样本（如给定不同prompt），人工评估生成质量。
**涉及技能**: 生成评估
**预期成果**: 生成样本合理，无明显语法错误。

## Day 12 Week 2, Day 5 (Friday)
**当日总时长**: 8 小时

**休息时间安排**: 每2小时休息10分钟，中午休息1小时。

**学习建议**: 文档要清晰，方便他人复现；总结时重点突出项目亮点。

### 当日任务
#### 撰写项目报告 (3小时)
**任务描述**: 撰写详细的项目报告，包括目标、模型架构、训练配置、实验结果、问题与解决方案、未来改进。
**涉及技能**: 文档撰写
**预期成果**: 完整的技术报告文档。

#### 整理代码 (2小时)
**任务描述**: 清理代码，添加注释，确保模块化，删除调试代码。
**涉及技能**: 代码整理
**预期成果**: 整洁、可读、可复用的代码库。

#### 创建GitHub仓库 (1小时)
**任务描述**: 初始化Git仓库，上传代码和报告，编写README（包含安装、训练、推理指南）。
**涉及技能**: 版本控制
**预期成果**: 公开的GitHub仓库，包含所有必要文件。

#### 总结双周学习 (1小时)
**任务描述**: 回顾两周所学知识，列出掌握的关键技能点，反思不足之处。
**涉及技能**: 总结
**预期成果**: 清晰的知识清单和反思笔记。

#### 准备下一阶段 (1小时)
**任务描述**: 预习下一双周计划（BERT/GPT预训练等），了解所需背景知识。
**涉及技能**: 计划
**预期成果**: 明确下一阶段的学习目标和资源。

## Day 13 Week 2, Day 6 (Saturday)
**当日总时长**: 8 小时

**休息时间安排**: 灵活休息。

**学习建议**: 模拟面试时注意表达清晰，结合项目经验回答。

### 当日任务
#### 补充学习 (4小时)
**任务描述**: 补全之前未完成的任务，深入学习CS224N剩余讲座（如BERT、GPT），阅读相关论文，如'Language Models are Unsupervised Multitask Learners'。
**涉及技能**: 拓展知识
**预期成果**: 完善知识体系，掌握GPT预训练的基本思想。

#### 模拟面试与复盘 (4小时)
**任务描述**: 查找大模型算法工程师常见面试题，进行模拟面试练习，并录音自我评估；同时对整个项目进行复盘，思考如何回答项目相关的问题。
**涉及技能**: 面试准备, 总结
**预期成果**: 整理面试常见问题及回答要点，能够流畅地介绍项目经验。

## Day 14 Week 2, Day 7 (Sunday)
**当日总时长**: 2 小时

**休息时间安排**: 全天以休息为主。

**学习建议**: 劳逸结合，保持学习热情。

### 当日任务
#### 总结与规划 (1小时)
**任务描述**: 总结本周学习成果，规划下一周的学习任务，调整计划。
**涉及技能**: 总结, 规划
**预期成果**: 清晰的总结文档和下周计划。

#### 休息与放松 (1小时)
**任务描述**: 进行休闲活动，确保精神恢复。
**涉及技能**: 休息
**预期成果**: 身心放松，准备新一周。

## 双周学习总结和回顾建议
经过两周的系统学习，你已经实现了从零构建一个Transformer decoder-only语言模型，完成了数据处理、模型搭建、训练、分布式训练、混合精度优化以及评估的全过程。你不仅巩固了PyTorch高级特性和Transformer理论基础，还获得了实际项目经验，包括代码编写、调试、性能优化和文档撰写。下一步将进入大模型预训练与微调的学习。建议回顾整个项目，确保所有指标达到目标，并准备好进入下一个双周计划。